{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9f6b2f",
   "metadata": {},
   "source": [
    "RUNNable is core abstraction representing composable units of work:(invoked, batched, streamed, transformed and composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33c9dc",
   "metadata": {},
   "source": [
    "Runnable Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868322a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, hereâ€™s a five-line essay about cricket, aiming for a balance of detail and engaging language:\\n\\nCricket, a game steeped in history and ritual, is a captivating blend of strategy and athleticism.  Played across continents with intricate rules, it demands precision and teamwork, requiring careful observation and a swift, decisive throw.  The rhythmic beat of the bat against the leather creates a hypnotic pulse, while the debate over whether a ball is alive fuels passionate moments.  More than just a sport, it's a cultural tradition, a testament to centuries of storytelling and camaraderie.  From humble beginnings in the British countryside, cricket continues to draw millions, a timeless game of skill and connection.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"Generate 5 line essay about the {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "parser=StrOutputParser()\n",
    "\n",
    "model=ChatOpenAI(\n",
    "    model='ai/gemma3:1B-Q4_K_M',\n",
    "    base_url=\"http://localhost:12434/engines/v1\",\n",
    "    api_key=\"docker\"\n",
    ")\n",
    "\n",
    "runnable_sequence=RunnableSequence(prompt1,model,parser)\n",
    "\n",
    "runnable_sequence.invoke({\"topic\":\"Cricket\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e36c1",
   "metadata": {},
   "source": [
    "Runnable Parallel (same input,but process independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5845b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a three-sentence Twitter post AI:\n",
      "\n",
      "**Generate engaging and creative tweets based on your input! Just tell me the topic, tone, and desired length, and we'll craft a compelling tweet for you.**\n",
      "\n",
      "**Weâ€™ll deliver tweets that spark conversation, build your brand, or simply add a little personality to your feed.**\n",
      "\n",
      "**Letâ€™s get started! What would you like me to tweet about?** ðŸš€\n",
      "Okay, here's a 3-sentence LinkedIn post about AI:\n",
      "\n",
      "AI is rapidly transforming industries, offering incredible potential for innovation and efficiency. From automating tasks to uncovering valuable insights, AI is becoming increasingly integrated into our daily lives.  Letâ€™s explore how AI can help you leverage its power to achieve your goals! #AI #ArtificialIntelligence #Innovation \n",
      "\n",
      "---\n",
      "\n",
      "**To help me tailor it even better, could you tell me:**\n",
      "\n",
      "*   **What's the overall tone you're aiming for?** (e.g., informative, enthusiastic, thought-provoking, practical)\n",
      "*   **Who is your target audience?** (e.g., business leaders, marketers, students, general public?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel,RunnableSequence\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"Generate twitter post {topic} in 3 sentences\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"Generate LinkedIN post {topic} in 3 sentences\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "model=ChatOpenAI(\n",
    "    model='ai/gemma3:1B-Q4_K_M',\n",
    "    base_url=\"http://localhost:12434/engines/v1\",\n",
    "    api_key=\"docker\"\n",
    ")\n",
    "parser=StrOutputParser()\n",
    "\n",
    "parallel_run=RunnableParallel(\n",
    "    {\n",
    "    \"twitter\":RunnableSequence(prompt1,model,parser),\n",
    "    \"Linkedin\":RunnableSequence(prompt2,model,parser)\n",
    "}\n",
    ")\n",
    "\n",
    "res=parallel_run.invoke({\"topic\":\"AI\"})\n",
    "\n",
    "print(res[\"twitter\"])\n",
    "print(res[\"Linkedin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f36ba1",
   "metadata": {},
   "source": [
    "Runnable Lambda(can give an function in chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84c3328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def count_word(text):\n",
    "    return len(text.split())\n",
    "\n",
    "runnable_count_word=RunnableLambda(count_word)\n",
    "\n",
    "runnable_count_word.invoke(\"hi, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab405e",
   "metadata": {},
   "source": [
    "Combined example for RunnableLambda,RunnableParallel,RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a91b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a joke for you about AI:\n",
      "\n",
      "Why did the AI cross the road?\n",
      "\n",
      "... To get to the other side of the data! \n",
      "\n",
      "---\n",
      "\n",
      "Do you want another one? ðŸ˜Š \n",
      " Word count32\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda,RunnableParallel,RunnableSequence,RunnablePassthrough\n",
    "\n",
    "def count_word(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Tell me a joke about the {topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "parser=StrOutputParser()\n",
    "\n",
    "model=ChatOpenAI(\n",
    "    model='ai/gemma3:1B-Q4_K_M',\n",
    "    base_url=\"http://localhost:12434/engines/v1\",\n",
    "    api_key=\"docker\"\n",
    ")\n",
    "\n",
    "joke_run=RunnableSequence(prompt,model,parser)\n",
    "\n",
    "parallel_run=RunnableParallel(\n",
    "    {\n",
    "        \"joke\":RunnablePassthrough(),\n",
    "        \"count\":RunnableLambda(count_word)\n",
    "    }\n",
    ")\n",
    " \n",
    "final_sequence=RunnableSequence(joke_run,parallel_run)\n",
    "\n",
    "res=final_sequence.invoke({\"topic\":\"AI\"})\n",
    "\n",
    "final_res=\"\"\"{} \\n Word count{}\"\"\".format(res['joke'],res['count'])\n",
    "\n",
    "print(final_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8cead",
   "metadata": {},
   "source": [
    "Runnable PAss Through(same input---same output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c429ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"Okay, here's a cricket joke for you:\\n\\nWhy did the cricket cross the playground?\\n\\n... To get to the other slide! \\n\\n---\\n\\nWould you like another one? ðŸ˜Š\", 'explanation': 'Okay, hereâ€™s another cricket joke for you:\\n\\nWhy did the cricket cross the playground? \\n\\n... To get to the other slide! \\n\\n---\\n\\nAbsolutely! Would you like another one? ðŸ˜Š \\n\\nLet me know if youâ€™d like a different style â€“ perhaps a pun, a riddle, or a short story!'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser=StrOutputParser()\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"write a joke on the {topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt12=PromptTemplate(\n",
    "    template=\"explain  the following joke {text}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'cricket'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752132fe",
   "metadata": {},
   "source": [
    "Runnable Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4b652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, this is a *very* comprehensive report. Itâ€™s excellent! To help me refine it further, let's focus on a specific angle.  \n",
      "\n",
      "**I'd like to know: Should I prioritize a deeper analysis of the *cyber warfare* aspect of the conflict, or would you prefer I focus more on the *economic impact* on Europe, specifically regarding energy markets and trade?**\n",
      "\n",
      "Let's say, for now, I want to delve deeper into the **economic impact of the war on Europe, specifically focusing on energy markets and trade.**  Could you help me shape the report to emphasize this point?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model=ChatOpenAI(\n",
    "    model='ai/gemma3:1B-Q4_K_M',\n",
    "    base_url=\"http://localhost:12434/engines/v1\",\n",
    "    api_key=\"docker\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f834b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
